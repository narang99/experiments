{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe358927-71ef-414e-9439-c8b049cf1823",
   "metadata": {},
   "source": [
    "---\n",
    "title: How long do I descend\n",
    "description: Finding how many steps it takes for gradient descent to converge\n",
    "author: Hariom  Narang\n",
    "date: 'January 6, 2026'\n",
    "image: images/gradient-descent-tumble-meme.webp\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550e510-7314-499e-abc4-2f622bdcdffc",
   "metadata": {},
   "source": [
    "In this article, I'm going to investigate how many steps gradient descent takes in linear regression, for a randomly chosen starting point.   \n",
    "I'm assuming a good starting point is very useful while training AI models, although until now, I've generally started from random weights.  For now, this is an attempt to understand the relationship between the starting weights and the number of iterations gradient descent takes to converge.  \n",
    "This article does this analysis for the simplest case of linear regression. We assume prior knowledge of linear regression and basic college math.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91051e4b-f3a2-4b42-9cc7-0e03169d1cc9",
   "metadata": {},
   "source": [
    "## Experiment \n",
    "\n",
    "Given:\n",
    "- starting weight `w0`\n",
    "- sample of points `(x,y)`\n",
    "- a learning rate `L`\n",
    "\n",
    "Find:\n",
    "How many iterations does gradient descent take to converge?  \n",
    "We do it for the special case of modelling `y=mx` (linear regression) for now   \n",
    "\n",
    "Strategy:\n",
    "Find the equation for `loss` at `k`th iteration in terms of the parameters given above (mainly, we want to model it on the starting weight) \n",
    "\n",
    "\n",
    "> The more general question here is modelling how gradient descent (or any other kind of descent) converges for arbitrary functions, inputs and outputs.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23731e67-87b1-46e0-b2b0-a7bff9380b9d",
   "metadata": {},
   "source": [
    "## Classic Linear Regression\n",
    "\n",
    "I'm defining the classic and the simplest linear regression problem. I assume you know linear regression though.  \n",
    "\n",
    "Given a set of points $\\{(x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)\\}$, we want to find a function $y = mx$ which minimizes the mean squared error (called $loss$ here, which we want to minimize)   \n",
    "$$ loss = \\sum_{i=1}^{n} (y_i - mx_i)^2  $$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "We start by choosing a random $m = w = w_0$.   \n",
    "At each step we update $w$ to a new value with the formula as below:  \n",
    "$$ G = \\frac{d}{dw}loss \\;=\\; \\sum_{i=1}^{n} 2x_i(wx_i - y_i) $$\n",
    "$$ w' = w -  lG \\;=\\; w - 2l\\sum_{i=1}^{n} x_i(wx_i - y_i) $$\n",
    "$$ w' = w - L\\sum_{i=1}^{n} x_i(wx_i - y_i) \\qquad (L := 2l) $$\n",
    "\n",
    "> The substitution $L = 2l$ is simply changing one constant to another (the $2$ is not very useful)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe1b9b-8817-48cb-8d28-ac3ca504078e",
   "metadata": {},
   "source": [
    "Let's start by expanding the equations for the next weight calculation $w'$ and the $loss$ function\n",
    "\n",
    "$$ w' = w - L\\sum_{i=1}^{n} (wx_i^2 - x_iy_i) \\;=\\; w - Lw\\sum_{i=1}^{n} x_i^2  + L\\sum_{i=1}^{n} x_iy_i $$\n",
    "$$ \n",
    "loss = \\sum_{i=1}^{n} (y_i - wx_i)^2  \\;=\\; \n",
    "   \\sum_{i=1}^{n}(y_i^2 + w^2x_i^2 - 2y_ixi_w) \\;=\\; \n",
    "   \\sum_{i=1}^{n} y_i^2 + \\sum_{i=1}^{n}w^2x_i^2 - \\sum_{i=1}^{n}2wx_iy_i\n",
    "$$\n",
    "\n",
    "\n",
    "The terms $\\sum_{i=1}^{n} x_i^2 \\;, \\sum_{i=1}^{n} x_iy_i \\;, \\sum_{i=1}^{n} y_i^2$ are constants for a given sample of points.   \n",
    "They will also keep coming everywhere, we will for now substitute them with new symbols.  \n",
    "$$ \\alpha =  \\sum_{i=1}^{n} x_i^2 \\quad \\beta = \\sum_{i=1}^{n} x_iy_i \\quad \\gamma = \\sum_{i=1}^{n} y_i^2 $$\n",
    "\n",
    "This gives us \n",
    "$$ w' = w - Lw\\alpha + L\\beta $$\n",
    "$$ loss = \\gamma + w^2\\alpha - 2w\\beta $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778197ce-cb2f-4003-8e19-ec2ea8d01359",
   "metadata": {},
   "source": [
    "## Loss at $k^{th}$ step\n",
    "The goal is to model the loss function in terms of the weight $w_0$ (the weight that we start gradient descent from) and the step $k$ of gradient descent.  \n",
    "I will use $w$ to denote $w_0$ (it's easier to write equations, it comes up everywhere)\n",
    "\n",
    "The loss after the first step ($k = 1$) in gradient descent would be \n",
    "$$ loss_1 = \\gamma + w_1^2\\alpha - 2w_1\\beta $$\n",
    "$$ \\text{where} \\; w_1 = w - Lw\\alpha + L\\beta $$\n",
    "\n",
    "Substitute $w_1$ in loss\n",
    "$$ loss_1 = \\gamma + (w - Lw\\alpha + L\\beta)^2\\alpha - 2(w - Lw\\alpha + L\\beta)\\beta $$\n",
    "\n",
    "This is quite a lot of work (which was done in a notebook before).  \n",
    "We have to do this expansion successively for different $k$ values.  \n",
    "It's a lot of manual work and it's best to use code for this.  \n",
    "I'll be using `sympy` to expand polynomials now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc57dae-3593-4dbb-a0c2-3af2dfc8bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff874be-04c8-4686-8e8f-cf460696d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all variables\n",
    "\n",
    "x = IndexedBase('x')\n",
    "y = IndexedBase('y')\n",
    "n = symbols('n')\n",
    "i = symbols('i', cls=Idx)\n",
    "\n",
    "\n",
    "alpha, beta, gamma = symbols('alpha beta gamma')\n",
    "w, L, k = symbols('w L k')\n",
    "\n",
    "sum_rg = (i,0,n)\n",
    "\n",
    "max_k = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae57bc-a0e1-421c-ad90-fe81da061b77",
   "metadata": {},
   "source": [
    "Our core equations using `sympy`.  \n",
    "`sympy` works on math equations. I'll make it do polynomial expansion (and also modelling the final expression as a polynomial with $w$ as a free variable)   \n",
    "\n",
    "The functions below are used for generating these equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666fe241-523c-4e0e-ac20-bcd987836554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given w, express w at next step\n",
    "def next_w(w_expr):\n",
    "    return -L*alpha*w_expr + L*beta + w_expr\n",
    "\n",
    "# express loss for given w expression\n",
    "def loss_at_w(w_expr):\n",
    "    return alpha*w_expr**2 - 2*beta*w_expr + gamma\n",
    "\n",
    "# make it a polynomial\n",
    "def Pw(exp):\n",
    "    return Poly(exp, w)\n",
    "\n",
    "# return as an expression, useful for showing results\n",
    "def E(poly):\n",
    "    return poly.as_expr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13ccab02-ba73-4dbf-803f-fe9c3cd24fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - L \\alpha w + L \\beta + w$"
      ],
      "text/plain": [
       "-L*alpha*w + L*beta + w"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\alpha w^{2} - 2 \\beta w + \\gamma$"
      ],
      "text/plain": [
       "alpha*w**2 - 2*beta*w + gamma"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle L^{2} \\alpha \\beta^{2} - 2 L \\beta^{2} + \\gamma + w^{2} \\left(L^{2} \\alpha^{3} - 2 L \\alpha^{2} + \\alpha\\right) + w \\left(- 2 L^{2} \\alpha^{2} \\beta + 4 L \\alpha \\beta - 2 \\beta\\right)$"
      ],
      "text/plain": [
       "L**2*alpha*beta**2 - 2*L*beta**2 + gamma + w**2*(L**2*alpha**3 - 2*L*alpha**2 + alpha) + w*(-2*L**2*alpha**2*beta + 4*L*alpha*beta - 2*beta)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show w1 in the form of w0\n",
    "w_1 = next_w(w)\n",
    "display(w_1)\n",
    "\n",
    "# loss given a w0\n",
    "loss_0 = loss_at_w(w)\n",
    "display(E(Pw(loss_0)))\n",
    "\n",
    "# loss at w1\n",
    "loss_1 = loss_at_w(w_1)\n",
    "display(E(Pw(loss_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92287464-7a06-46a8-b96c-dbdc41d69491",
   "metadata": {},
   "source": [
    "We will use `sympy` to the equations for the first few iterations.  \n",
    "The cell below stores all expressions for deriving $w_k$ given $w = w_0$ in the variable `w_at_k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe20333-dba0-44b9-907d-9d5d1a0098c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {W}_{0} = w$"
      ],
      "text/plain": [
       "Eq(W[0], w)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {W}_{1} = - L \\alpha w + L \\beta + w$"
      ],
      "text/plain": [
       "Eq(W[1], -L*alpha*w + L*beta + w)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {W}_{2} = - L \\alpha w - L \\alpha \\left(- L \\alpha w + L \\beta + w\\right) + 2 L \\beta + w$"
      ],
      "text/plain": [
       "Eq(W[2], -L*alpha*w - L*alpha*(-L*alpha*w + L*beta + w) + 2*L*beta + w)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_all_ws(current_w, n=max_k):\n",
    "    r = []\n",
    "    wi = current_w\n",
    "    for i in range(n):\n",
    "        r.append(wi)\n",
    "        wi = next_w(wi)\n",
    "    return r\n",
    "\n",
    "\n",
    "w_at_k = compute_all_ws(w)\n",
    "\n",
    "# print the first three steps\n",
    "W = IndexedBase('W')\n",
    "for i in range(3):\n",
    "    display(Eq(W[i], w_at_k[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3243b-363f-4b2c-90f4-3429e8977d9c",
   "metadata": {},
   "source": [
    "Calculate the loss expressions using `w_at_k` and store in `losses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "770142e4-a55d-401f-9043-e07cca667540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Loss}_{0} = \\alpha w^{2} - 2 \\beta w + \\gamma$"
      ],
      "text/plain": [
       "Eq(Loss[0], alpha*w**2 - 2*beta*w + gamma)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Loss}_{1} = L^{2} \\alpha \\beta^{2} - 2 L \\beta^{2} + \\gamma + w^{2} \\left(L^{2} \\alpha^{3} - 2 L \\alpha^{2} + \\alpha\\right) + w \\left(- 2 L^{2} \\alpha^{2} \\beta + 4 L \\alpha \\beta - 2 \\beta\\right)$"
      ],
      "text/plain": [
       "Eq(Loss[1], L**2*alpha*beta**2 - 2*L*beta**2 + gamma + w**2*(L**2*alpha**3 - 2*L*alpha**2 + alpha) + w*(-2*L**2*alpha**2*beta + 4*L*alpha*beta - 2*beta))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Loss}_{2} = L^{4} \\alpha^{3} \\beta^{2} - 4 L^{3} \\alpha^{2} \\beta^{2} + 6 L^{2} \\alpha \\beta^{2} - 4 L \\beta^{2} + \\gamma + w^{2} \\left(L^{4} \\alpha^{5} - 4 L^{3} \\alpha^{4} + 6 L^{2} \\alpha^{3} - 4 L \\alpha^{2} + \\alpha\\right) + w \\left(- 2 L^{4} \\alpha^{4} \\beta + 8 L^{3} \\alpha^{3} \\beta - 12 L^{2} \\alpha^{2} \\beta + 8 L \\alpha \\beta - 2 \\beta\\right)$"
      ],
      "text/plain": [
       "Eq(Loss[2], L**4*alpha**3*beta**2 - 4*L**3*alpha**2*beta**2 + 6*L**2*alpha*beta**2 - 4*L*beta**2 + gamma + w**2*(L**4*alpha**5 - 4*L**3*alpha**4 + 6*L**2*alpha**3 - 4*L*alpha**2 + alpha) + w*(-2*L**4*alpha**4*beta + 8*L**3*alpha**3*beta - 12*L**2*alpha**2*beta + 8*L*alpha*beta - 2*beta))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_losses(ws):\n",
    "    return [loss_at_w(w) for w in ws]\n",
    "\n",
    "losses = compute_losses(w_at_k)\n",
    "LOSS = IndexedBase('Loss')\n",
    "for i in range(3):\n",
    "    display(Eq(LOSS[i], E(Pw(losses[i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4356bc6-b576-47c4-9f7b-f59077feec08",
   "metadata": {},
   "source": [
    "## Factorizing\n",
    "\n",
    "It's time to factorize these equations, or find a pattern in the function for loss.  \n",
    "The loss function is a quadratic polynomial in terms of $w$ (the starting weight) for all $k$.   \n",
    "\n",
    "My strategy is to just expand all polynomials for many $k$ values and try to figure out a pattern.  \n",
    "After that, I will computationally verify if our simplifications are correct for some $k$ values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec489ebf-32e9-41e1-85fc-b391392b28d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Finding coefficient of $w^2$ \n",
    "Let's start with the coefficient for $w^2$  \n",
    "\n",
    "I will be writing the coefficients of $w^2$ now for the first few iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147d2711-7887-4523-a7e4-fe67fd01a25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{0} = \\alpha$"
      ],
      "text/plain": [
       "Eq(Coeff[0], alpha)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{1} = \\alpha \\left(L^{2} \\alpha^{2} - 2 L \\alpha + 1\\right)$"
      ],
      "text/plain": [
       "Eq(Coeff[1], alpha*(L**2*alpha**2 - 2*L*alpha + 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{2} = \\alpha \\left(L^{4} \\alpha^{4} - 4 L^{3} \\alpha^{3} + 6 L^{2} \\alpha^{2} - 4 L \\alpha + 1\\right)$"
      ],
      "text/plain": [
       "Eq(Coeff[2], alpha*(L**4*alpha**4 - 4*L**3*alpha**3 + 6*L**2*alpha**2 - 4*L*alpha + 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{3} = \\alpha \\left(L^{6} \\alpha^{6} - 6 L^{5} \\alpha^{5} + 15 L^{4} \\alpha^{4} - 20 L^{3} \\alpha^{3} + 15 L^{2} \\alpha^{2} - 6 L \\alpha + 1\\right)$"
      ],
      "text/plain": [
       "Eq(Coeff[3], alpha*(L**6*alpha**6 - 6*L**5*alpha**5 + 15*L**4*alpha**4 - 20*L**3*alpha**3 + 15*L**2*alpha**2 - 6*L*alpha + 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{4} = \\alpha \\left(L^{8} \\alpha^{8} - 8 L^{7} \\alpha^{7} + 28 L^{6} \\alpha^{6} - 56 L^{5} \\alpha^{5} + 70 L^{4} \\alpha^{4} - 56 L^{3} \\alpha^{3} + 28 L^{2} \\alpha^{2} - 8 L \\alpha + 1\\right)$"
      ],
      "text/plain": [
       "Eq(Coeff[4], alpha*(L**8*alpha**8 - 8*L**7*alpha**7 + 28*L**6*alpha**6 - 56*L**5*alpha**5 + 70*L**4*alpha**4 - 56*L**3*alpha**3 + 28*L**2*alpha**2 - 8*L*alpha + 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, l in enumerate(losses[:5]):\n",
    "    term = Pw(l).coeff_monomial(w*w)\n",
    "    term = cancel(term / alpha)\n",
    "    COEFF = IndexedBase(\"Coeff\")\n",
    "    display(Eq(COEFF[i], term * alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8665b-9fe8-4ed8-9e46-0337eb1d30a0",
   "metadata": {},
   "source": [
    "The pattern is actually pretty simple, the coefficient is of the form\n",
    "\n",
    "$$ Coeff_k = \\alpha(L\\alpha - 1)^{2k} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c58cd0-0133-4a2d-bca7-cd538d0a3514",
   "metadata": {},
   "source": [
    "### Finding coefficient of $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c08e3ffc-b74a-48dc-9892-7f8f7c629f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{0} = - 2 \\beta$"
      ],
      "text/plain": [
       "Eq(Coeff[0], -2*beta)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{1} = - 2 \\beta \\left(L^{2} \\alpha^{2} - 2 L \\alpha + 1\\right)$"
      ],
      "text/plain": [
       "Eq(Coeff[1], -2*beta*(L**2*alpha**2 - 2*L*alpha + 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{2} = - 2 \\beta \\left(L^{4} \\alpha^{4} - 4 L^{3} \\alpha^{3} + 6 L^{2} \\alpha^{2} - 4 L \\alpha + 1\\right)$"
      ],
      "text/plain": [
       "Eq(Coeff[2], -2*beta*(L**4*alpha**4 - 4*L**3*alpha**3 + 6*L**2*alpha**2 - 4*L*alpha + 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{3} = - 2 \\beta \\left(L^{6} \\alpha^{6} - 6 L^{5} \\alpha^{5} + 15 L^{4} \\alpha^{4} - 20 L^{3} \\alpha^{3} + 15 L^{2} \\alpha^{2} - 6 L \\alpha + 1\\right)$"
      ],
      "text/plain": [
       "Eq(Coeff[3], -2*beta*(L**6*alpha**6 - 6*L**5*alpha**5 + 15*L**4*alpha**4 - 20*L**3*alpha**3 + 15*L**2*alpha**2 - 6*L*alpha + 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{4} = - 2 \\beta \\left(L^{8} \\alpha^{8} - 8 L^{7} \\alpha^{7} + 28 L^{6} \\alpha^{6} - 56 L^{5} \\alpha^{5} + 70 L^{4} \\alpha^{4} - 56 L^{3} \\alpha^{3} + 28 L^{2} \\alpha^{2} - 8 L \\alpha + 1\\right)$"
      ],
      "text/plain": [
       "Eq(Coeff[4], -2*beta*(L**8*alpha**8 - 8*L**7*alpha**7 + 28*L**6*alpha**6 - 56*L**5*alpha**5 + 70*L**4*alpha**4 - 56*L**3*alpha**3 + 28*L**2*alpha**2 - 8*L*alpha + 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, l in enumerate(losses[:5]):\n",
    "    term = Pw(l).coeff_monomial(w)\n",
    "    term = cancel(term / (-2*beta))\n",
    "    COEFF = IndexedBase(\"Coeff\")\n",
    "    display(Eq(COEFF[i], term * (-2*beta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26a550-ecb7-4a56-a7a4-d2492ef9c483",
   "metadata": {},
   "source": [
    "Luckily this pattern is also of the form $(L\\alpha - 1)^{2k}$\n",
    "\n",
    "$$ Coeff_k = -2\\beta(L\\alpha-1)^{2k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2101c2-892e-4517-a68e-93b45fbfe826",
   "metadata": {},
   "source": [
    "### Finding the constant term\n",
    "\n",
    "The final constant coefficient is, unfortunately, not as simple as the above coefficients. It took some experimentation from my side.   \n",
    "I'm adding the method that gave me the final simplified form. I used a mixture of solving manually and using `sympy` to find this expression.  \n",
    "Let's first print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46e71993-0d66-464e-b9d6-f07f2aab55d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{0} = \\gamma$"
      ],
      "text/plain": [
       "Eq(Coeff[0], gamma)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{1} = L \\beta^{2} \\left(L \\alpha - 2\\right) + \\gamma$"
      ],
      "text/plain": [
       "Eq(Coeff[1], L*beta**2*(L*alpha - 2) + gamma)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{2} = L \\beta^{2} \\left(L^{3} \\alpha^{3} - 4 L^{2} \\alpha^{2} + 6 L \\alpha - 4\\right) + \\gamma$"
      ],
      "text/plain": [
       "Eq(Coeff[2], L*beta**2*(L**3*alpha**3 - 4*L**2*alpha**2 + 6*L*alpha - 4) + gamma)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {Coeff}_{3} = L \\beta^{2} \\left(L^{5} \\alpha^{5} - 6 L^{4} \\alpha^{4} + 15 L^{3} \\alpha^{3} - 20 L^{2} \\alpha^{2} + 15 L \\alpha - 6\\right) + \\gamma$"
      ],
      "text/plain": [
       "Eq(Coeff[3], L*beta**2*(L**5*alpha**5 - 6*L**4*alpha**4 + 15*L**3*alpha**3 - 20*L**2*alpha**2 + 15*L*alpha - 6) + gamma)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, l in enumerate(losses[:4]):\n",
    "    term = Pw(l).coeff_monomial(1)\n",
    "    term = cancel((term - gamma) / (L*beta*beta))\n",
    "    COEFF = IndexedBase(\"Coeff\")\n",
    "    display(Eq(COEFF[i], L*beta*beta*term + gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d94fc-80d9-45f9-9708-ac027375460e",
   "metadata": {},
   "source": [
    "The sub-expression inside $L\\beta^2$ does look very similar to $(L\\alpha - 1)^{2k}$. The coefficients are jumbled though.  \n",
    "I tried writing this sub-expression as a form of $(L\\alpha - 1)^{2k}$. After some experimentation, I got to the correct form the sub-expression takes.  \n",
    "I'll show the form for $Coeff_2$ directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d41d93-7ed0-4a0b-8a10-ef5e2fbe4efb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "Sub_2 \n",
    "&= (L^{3} \\alpha^{3}  - 4 L^{2} \\alpha^{2} + 6 L \\alpha - 4 ) \\\\\n",
    "&= (L^{3} \\alpha^{3} - 3 L^{2} \\alpha^{2} + 3 L \\alpha - 1) - (L^2\\alpha^2 - 3L\\alpha + 3)  \\\\\n",
    "&= (L^{3} \\alpha^{3} - 3 L^{2} \\alpha^{2} + 3 L \\alpha - 1) - (L^2\\alpha^2 - 2L\\alpha + 1) + (L\\alpha - 2)  \\\\\n",
    "&= (L^{3} \\alpha^{3} - 3 L^{2} \\alpha^{2} + 3 L \\alpha - 1) - (L^2\\alpha^2 - 2L\\alpha + 1) + (L\\alpha - 1) - 1  \\\\\n",
    "&=  (L\\alpha - 1)^3 - (L\\alpha - 1)^2 + (L\\alpha - 1) - 1 \\\\\n",
    "&= -\\left[-(L\\alpha - 1)^3 + (L\\alpha - 1)^2 - (L\\alpha - 1) + 1\\right] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "The last expression is a geometric series with respect to $(L\\alpha -1)$, with $a=1$and$r=-(L\\alpha - 1)$. The last power is $ 2k - 1$  \n",
    "The [sum](https://math.stackexchange.com/questions/4255628/proof-of-geometric-series-formula) of a finite geometric series\n",
    "$$ x^n + \\; ... \\; + x^2 + x + 1 = \\frac{(1 - x^{n+1})}{(1-x)}  $$\n",
    "For us $x := -(L\\alpha - 1)$ and $n = 2k-1$.   \n",
    "\n",
    "Calculating the sum\n",
    "$$ S \\;=\\; \\frac{1 - (1-L\\alpha)^{2k}}{1 - (1-L\\alpha)} \\;=\\;  \n",
    "\\frac{1 - (1-L\\alpha)^{2k}}{L\\alpha} \\;=\\;  \n",
    "\\frac{1 - (L\\alpha-1)^{2k}}{L\\alpha} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "Sub_2 \n",
    "&= -\\left[-(L\\alpha - 1)^3 + (L\\alpha - 1)^2 - (L\\alpha - 1) + 1\\right]  \\\\\n",
    "&= -S \\\\\n",
    "&= -\\left(\\frac{1 - (L\\alpha-1)^{2k}}{L\\alpha}\\right) \\\\\n",
    "&= \\frac{(L\\alpha-1)^{2k} - 1}{L\\alpha} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb719a-6d54-4f25-82b8-372988572ff7",
   "metadata": {},
   "source": [
    "The constant coefficient can now be written as\n",
    "$$ \\gamma + L\\beta^2\\left(\\frac{1 - (L\\alpha-1)^{2k}}{L\\alpha}\\right) \\;=\\; \\gamma + \\beta^2\\left(\\frac{(L\\alpha-1)^{2k}-1}{\\alpha}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc84ac7-eb20-4157-9712-bc584d773f95",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Loss_k \n",
    "&= w^2\\left[\\alpha(L\\alpha - 1)^{2k}\\right]+ w\\left[-2\\beta(L\\alpha-1)^{2k}\\right]+ \\left[\\gamma + \\beta^2\\left(\\frac{(L\\alpha-1)^{2k}-1}{\\alpha}\\right)\\right] \\\\\n",
    "&= w^2\\left[\\alpha(L\\alpha - 1)^{2k}\\right]+ w\\left[-2\\beta(L\\alpha-1)^{2k}\\right] + \\frac{\\beta^2(L\\alpha-1)^{2k}}{\\alpha} + \\frac{\\alpha\\gamma - \\beta^2}{\\alpha} \\\\\n",
    "&= (L\\alpha - 1)^{2k}\\frac{\\alpha^2w^2 - 2\\alpha\\beta + \\beta^2}{\\alpha} + \\frac{\\alpha\\gamma - \\beta^2}{\\alpha} \\\\\n",
    "&= (L\\alpha - 1)^{2k}\\frac{(w\\alpha-\\beta)^2}{\\alpha} + \\frac{\\alpha\\gamma - \\beta^2}{\\alpha} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "So our final expression\n",
    "$$\n",
    "Loss_k = (L\\alpha - 1)^{2k}\\frac{(w\\alpha-\\beta)^2}{\\alpha} + \\frac{\\alpha\\gamma - \\beta^2}{\\alpha}\n",
    "$$\n",
    "\n",
    "The expanded form of this expression:\n",
    "$$\n",
    "Loss_k = \\frac{\\left(\\left(L \\sum_{i=0}^{n} {x}_{i}^{2} - 1\\right)^{2 k}\\right) \\left(w \\sum_{i=0}^{n} {x}_{i}^{2} - \\sum_{i=0}^{n} {x}_{i} {y}_{i}\\right)^{2} - \\left(\\sum_{i=0}^{n} {x}_{i} {y}_{i}\\right)^{2} + \\left(\\sum_{i=0}^{n} {x}_{i}^{2}\\right) \\sum_{i=0}^{n} {y}_{i}^{2}}{\\sum_{i=0}^{n} {x}_{i}^{2}}\n",
    "$$\n",
    "\n",
    "Doing a similar analysis for modelling $w$in terms of$k$ gives us\n",
    "\n",
    "$$w_k = \\frac{\\beta + (1-L\\alpha)^k(w\\alpha-b)}{\\alpha}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f17e31-b576-4339-b95c-bf2027c9bd29",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "- Loss is an exponential function in terms of $k$; and a quadratic function in terms of $w$.\n",
    "    - It does not matter what $w$ you start with, the first term rapidly becomes influenced by $k$\n",
    "    - If $| L\\alpha - 1 | > 1$, the loss will explode. Regression would not converge.\n",
    "- $\\frac{\\alpha\\gamma - \\beta^2}{\\alpha}$ is constant for a given sample, it does not matter on either $k$, $L$or$w$, I will call this $Loss_{base}$. Your loss cannot be lesser than this.\n",
    "- $w_k$converges to$\\beta / \\alpha$(this is the true value, we also come to this exact formula for the true value if we solve for$G=0$)\n",
    "- If $L\\alpha - 1 = 0$, the loss becomes minimum after the first step, no matter which $w$ you start from.\n",
    "    - The cool thing is that this is a constant and does not depend on $w$.\n",
    "    - I believe this is because gradient itself is a linear function directly proportional to $w$\n",
    "\n",
    "##### Cauchy-Schwarz Inequality\n",
    "\n",
    "Let's look at the loss function\n",
    "$$ Loss_k = (L\\alpha - 1)^{2k}\\frac{(w\\alpha-\\beta)^2}{\\alpha} + \\frac{\\alpha\\gamma - \\beta^2}{\\alpha} $$\n",
    "\n",
    "$L$here is a free variable, we will look at the special case of$L = \\frac{1}{\\alpha}$ (the first term becomes zero)\n",
    "\n",
    "$$ Loss_k = \\frac{\\alpha\\gamma - \\beta^2}{\\alpha} $$\n",
    "\n",
    "The original definition of loss from which we derived the above function is\n",
    "$$ loss = \\sum_{i=1}^{n} (y_i - mx_i)^2  $$ \n",
    "\n",
    "These two functions are equivalent for some $m$ \n",
    "$$ Loss_k = \\frac{\\alpha\\gamma - \\beta^2}{\\alpha} = \\sum_{i=1}^{n} (y_i - m_k x_i)^2 > 0 $$\n",
    "\n",
    "$$ \\frac{\\alpha\\gamma - \\beta^2}{\\alpha} > 0 $$\n",
    "$$ \\alpha\\gamma > \\beta^2 $$\n",
    "$$ \\sum_{i=0}^{n} {x}_{i}^{2} \\sum_{i=0}^{n} {y}_{i}^{2} > \\left(\\sum_{i=0}^{n} {x}_{i} {y}_{i}\\right)^{2} $$\n",
    "\n",
    "This seems to be an indirect proof of [Cauchy-Schwarz](https://artofproblemsolving.com/wiki/index.php/Cauchy-Schwarz_Inequality) Inequality in the elemantary algebraic form. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4f857-3888-451e-95ce-3765d6e2f325",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "I might be able to definitively prove this expression using induction. For now however, I'm going to expand my own expression using `sympy` and check if it is the same as the original expression.  \n",
    "\n",
    "I've also computationally verified this. I do raw gradient descent and see if the losses are matching. I'm not going to do it here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e1aed1-b698-4fb7-bca8-9514f1c09c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sympy\n",
    "\n",
    "def theory_loss_expr(k):\n",
    "    sub = (L*alpha - 1)**(2*k)\n",
    "    # expr = w*w*(alpha*sub) - w*(2*beta*sub) + (beta*beta/alpha)*(sub-1) + gamma\n",
    "    expr = (sub*((w*alpha - beta)**2)) + (alpha*gamma - beta*beta)\n",
    "    expr /= alpha\n",
    "    return expr\n",
    "\n",
    "# check if all losses are the same\n",
    "# this takes a few seconds to run\n",
    "for i, l in enumerate(losses):\n",
    "    if expand(theory_loss_expr(i)) != expand(l):\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}